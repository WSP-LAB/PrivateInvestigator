import argparse
from os import path

from concurrent.futures import ProcessPoolExecutor
from tqdm import tqdm
import numpy as np
import pickle
import yaml
import torch
from scipy.spatial.distance import cosine

from pii_leakage.arguments.model_args import ModelArgs
from pii_leakage.models.model_factory import ModelFactory
from private_investigator.pii_collector import PIICollector


def collect_prefix_pii_pairs_inner(texts, ground_truth, target):
    pii_collector = PIICollector(target)
    all_pairs = []
    all_piis = []
    for text in texts:
        piis = pii_collector.get_pii([text])
        train_piis = []
        for pii in piis:
            if target == 'phone':
                formatted_pii = pii.replace('(','').replace(')','').replace(' ','-').replace('.','-')
            else:
                formatted_pii = pii
            if formatted_pii in ground_truth:
                train_piis.append(pii)
        pii_index = 0
        for pii in train_piis:
            pii_index = text.find(pii, pii_index)
            prefix = text[:pii_index]
            if target == 'phone':
                formatted_pii = pii.replace('(','').replace(')','').replace(' ','-').replace('.','-')
            else:
                formatted_pii = pii
            all_pairs.append([prefix, formatted_pii])
            all_piis.append(formatted_pii)
    all_piis = list(set(all_piis))
    return all_pairs, all_piis

def collect_prefix_pii_pairs(model, dataset, target):
    # set paths
    our_path = f'private_investigator_pre/{model}-{dataset}.c_0.25.temp_1.0.{target}.yml'
    car_path = f'carlini_result/{model}-{dataset}.topk.temp_1.texts.yml'
    luk_path = f'lukas_result/{model}-{dataset}.temp_1.0.texts.yml'
    # if there are missing results, skip
    if not (path.isfile(our_path) and path.isfile(car_path)
            and path.isfile(luk_path)):
        return
    print(f'Collecting prefix-PII pairs for {model}-{dataset}-{target}')

    # load ground truth
    with open(f'piis/{dataset}_{target}_train.yml', 'r') as f:
        ground_truth = yaml.load(f, Loader=yaml.FullLoader)

    # load private investigator result
    with open(our_path, 'r') as f:
        result = yaml.load(f, Loader=yaml.FullLoader)
    result_prompts = result[1]
    result_texts = result[3]
    our_texts = []
    for prompt, texts in zip(result_prompts, result_texts):
        for text in texts:
            our_texts.append(prompt+text)
    ours_prefix_pii, ours_piis = collect_prefix_pii_pairs_inner(our_texts, ground_truth, target)

    # load carlini result
    with open(car_path, 'r') as f:
        car_texts = yaml.load(f, Loader=yaml.FullLoader)
    car_texts = ['<|endoftext|>' + text for text in car_texts]
    car_prefix_pii, car_piis = collect_prefix_pii_pairs_inner(car_texts, ground_truth, target)

    # load lukas result
    with open(luk_path, 'r') as f:
        luk_texts = yaml.load(f, Loader=yaml.FullLoader)
    luk_prefix_pii, luk_piis = collect_prefix_pii_pairs_inner(luk_texts, ground_truth, target)

    base_prefix_pii = car_prefix_pii + luk_prefix_pii
    base_piis = list(set(car_piis + luk_piis))

    print('collect exclusive pairs generated by private investigator')
    ours_dict = {}
    ours_exclusive_piis = []
    for prefix, pii in ours_prefix_pii:
        if pii not in base_piis:
            if pii not in ours_dict:
                ours_dict[pii] = [prefix]
            else:
                ours_dict[pii].append(prefix)
            if pii not in ours_exclusive_piis:
                ours_exclusive_piis.append(pii)

    print('Collect exclusive pairs generated by baselines')
    base_dict = {}
    for prefix, pii in base_prefix_pii:
        if pii not in ours_piis:
            if pii not in base_prefix_pii:
                base_dict[pii] = [prefix]
            else:
                base_dict[pii].append(prefix)

    print('Collect exclusive pairs in dataset')
    ground_dict = {}
    dataset_path = f"contextual_similarity/{dataset}_merged.txt"
    with open(dataset_path, "r") as f:
        body_text = f.read()
    train_texts = body_text.split("\n|$$|\n")
    ground_prefix_pii, _ = collect_prefix_pii_pairs_inner(train_texts, ours_exclusive_piis, target)
    for prefix, pii in ground_prefix_pii:
        if pii not in ground_dict:
            ground_dict[pii] = [prefix]
        else:
            ground_dict[pii].append(prefix)

    return ground_dict, ours_dict, base_dict



def get_latent_vectors(lm, pii_prefix_dict):
    pii_latent_dict = {}
    for pii, prefix_list in tqdm(pii_prefix_dict.items()):
        latent_list = []
        for text in prefix_list:
            inputs = lm._tokenizer(text, return_tensors="pt", truncation=True, max_length=1024)
            inputs = {k: v.to('cuda') for k, v in inputs.items()}

            with torch.no_grad():
                outputs = lm._lm(**inputs, output_hidden_states=True)

            hidden_states = outputs.hidden_states
            last_token_index = inputs['attention_mask'].sum(dim=1) - 1
            last_token_index = last_token_index.squeeze().item()

            latent = hidden_states[-2][0, last_token_index, :].cpu().numpy()
            latent_list.append(latent)
        pii_latent_dict[pii] = latent_list
    return pii_latent_dict


def get_train_piis(piis, ground_truth):
    train_piis = []
    for pii in piis:
        if pii in ground_truth:
            train_piis.append(pii)
    return train_piis


def get_cossim(target_vectors, true_vectors):
    cossim_s_list = []

    for pred_v in target_vectors:
        cossim_s = 0
        for true_v in true_vectors:
            cossim = 1 - cosine(pred_v, true_v)
            if cossim > cossim_s:
                cossim_s = cossim
        cossim_s_list.append(cossim_s)
    
    return cossim_s_list


def compute_cossim(ours_dict, base_dict, ground_dict):
    cossim_ours = []
    cossim_base = []

    base_vectors = []
    for _, value in base_dict.items():
        base_vectors.extend(value)

    count = 0
    for identifier in tqdm(list(ours_dict.keys())):
        # get cosine similarity between PV-GV
        pred_vectors = ours_dict[identifier]
        true_vectors = ground_dict[identifier]
        cossim_mean_ours = np.mean(get_cossim(pred_vectors, true_vectors))
        cossim_ours.append(cossim_mean_ours)

        # get cosine similarity between OV-GV
        cossim_list_base = get_cossim(base_vectors, true_vectors)
        cossim_mean_base = np.mean(cossim_list_base)
        cossim_base.append(cossim_mean_base)

        # check our cossim is larger than that of baselines
        if cossim_mean_ours > cossim_mean_base:
            count += 1

    return count / len(ours_dict), np.mean(cossim_ours), np.mean(cossim_base)


def compare_pii_perplexity(model, dataset, target):

    # get pii prefix pairs
    piiprefix_path = f'contextual_similarity/{model}-{dataset}-{target}.yml'
    if path.isfile(piiprefix_path):
        with open(piiprefix_path, 'r') as f:
            ground_dict, ours_dict, base_dict = yaml.load(f, Loader=yaml.FullLoader)
    else:
        result = collect_prefix_pii_pairs(model, dataset, target)
        if result:
            ground_dict, ours_dict, base_dict = result
        else:
            return ''
        with open(piiprefix_path, 'w') as f:
            yaml.dump([ground_dict, ours_dict, base_dict], f)

    # load model
    model_args = ModelArgs(architecture=model, model_ckpt=f'weights/{model}-{dataset}')
    lm = ModelFactory.from_model_args(model_args).load()

    print(f'Collecting latent vectors for {model}-{dataset}-{target}')
    print('get ground truth latent vectors')
    ground_latent_path = f'contextual_similarity/{model}-{dataset}-{target}_latent_ground.pkl'
    if path.isfile(ground_latent_path):
        with open(ground_latent_path, 'rb') as f:
            ground_latent = pickle.load(f)
    else:
        ground_latent = get_latent_vectors(lm, ground_dict)
        with open(ground_latent_path, 'wb') as f:
            pickle.dump(ground_latent, f)

    print('get our latent vectors')
    ours_latent_path = f'contextual_similarity/{model}-{dataset}-{target}_latent_ours.pkl'
    if path.isfile(ours_latent_path):
        with open(ours_latent_path, 'rb') as f:
            ours_latent = pickle.load(f)
    else:
        ours_latent = get_latent_vectors(lm, ours_dict)
        with open(ours_latent_path, 'wb') as f:
            pickle.dump(ours_latent, f)

    print('get baseline latent vectors')
    base_latent_path = f'contextual_similarity/{model}-{dataset}-{target}_latent_base.pkl'
    if path.isfile(base_latent_path):
        with open(base_latent_path, 'rb') as f:
            base_latent = pickle.load(f)
    else:
        base_latent = get_latent_vectors(lm, base_dict)
        with open(base_latent_path, 'wb') as f:
            pickle.dump(base_latent, f)

    print('compute latent similarity')
    rate, pv_gv, ov_gv = compute_cossim(ours_latent, base_latent, ground_latent)
    message = f'{model}-{dataset}-{target}\t rate: {rate}, PV-GV: {pv_gv}, OV-GV: {ov_gv}\n'
    print(message)
    return message


def main():
    parser = argparse.ArgumentParser(description="Compute cosine and L2 distance between predicted and ground truth latent vectors.")
    parser.add_argument("--model", type=str, required=True, choices=["gpt2", "gptneo", "openelm", "phi2"])
    parser.add_argument("--dataset", type=str, required=True, choices=["enron", "trec"])
    parser.add_argument("--target", type=str, required=True, choices=["phone", "email", "name"])
    args = parser.parse_args()
    message = compare_pii_perplexity(args.model, args.dataset, args.target)
    with open('contextual_similarity.txt', 'a') as f:
        f.write(message)

if __name__ == '__main__':
    main()
